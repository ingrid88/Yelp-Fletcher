{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up spaCy\n",
    "from spacy.en import English\n",
    "parser = English()\n",
    "\n",
    "# Test Data\n",
    "multiSentence = \"There is an art, it says, or rather, a knack to flying.\" \\\n",
    "                 \"The knack lies in learning how to throw yourself at the ground and miss.\" \\\n",
    "                 \"In the beginning the Universe was created. This has made a lot of people \"\\\n",
    "                 \"very angry and been widely regarded as a bad move.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy does tokenization, sentence recognition, part of speech tagging, lemmatization, dependency parsing, and named entity recognition all at once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# all you have to do to parse text is this:\n",
    "#note: the first time you run spaCy in a file it takes a little while to load up its modules\n",
    "parsedData = parser(multiSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: 640 There\n",
      "lowercased: 530 there\n",
      "lemma: 530 there\n",
      "shape: 489792 Xxxxx\n",
      "prefix: 2907 T\n",
      "suffix: 48458 ere\n",
      "log probability: -7.347356796264648\n",
      "Brown cluster id: 1918\n",
      "----------------------------------------\n",
      "original: 474 is\n",
      "lowercased: 474 is\n",
      "lemma: 488 be\n",
      "shape: 21581 xx\n",
      "prefix: 570 i\n",
      "suffix: 474 is\n",
      "log probability: -4.457748889923096\n",
      "Brown cluster id: 762\n",
      "----------------------------------------\n",
      "original: 523 an\n",
      "lowercased: 523 an\n",
      "lemma: 523 an\n",
      "shape: 21581 xx\n",
      "prefix: 469 a\n",
      "suffix: 523 an\n",
      "log probability: -6.014852046966553\n",
      "Brown cluster id: 3\n",
      "----------------------------------------\n",
      "original: 1630 art\n",
      "lowercased: 1630 art\n",
      "lemma: 1630 art\n",
      "shape: 28983 xxx\n",
      "prefix: 469 a\n",
      "suffix: 1630 art\n",
      "log probability: -9.584548950195312\n",
      "Brown cluster id: 633\n",
      "----------------------------------------\n",
      "original: 416 ,\n",
      "lowercased: 416 ,\n",
      "lemma: 416 ,\n",
      "shape: 416 ,\n",
      "prefix: 416 ,\n",
      "suffix: 416 ,\n",
      "log probability: -3.4549596309661865\n",
      "Brown cluster id: 4\n",
      "----------------------------------------\n",
      "original: 473 it\n",
      "lowercased: 473 it\n",
      "lemma: 473 it\n",
      "shape: 21581 xx\n",
      "prefix: 570 i\n",
      "suffix: 473 it\n",
      "log probability: -4.388050079345703\n",
      "Brown cluster id: 474\n",
      "----------------------------------------\n",
      "original: 880 says\n",
      "lowercased: 880 says\n",
      "lemma: 594 say\n",
      "shape: 53740 xxxx\n",
      "prefix: 1012 s\n",
      "suffix: 338316 ays\n",
      "log probability: -8.426565170288086\n",
      "Brown cluster id: 244\n",
      "----------------------------------------\n",
      "original: 416 ,\n",
      "lowercased: 416 ,\n",
      "lemma: 416 ,\n",
      "shape: 416 ,\n",
      "prefix: 416 ,\n",
      "suffix: 416 ,\n",
      "log probability: -3.4549596309661865\n",
      "Brown cluster id: 4\n",
      "----------------------------------------\n",
      "original: 505 or\n",
      "lowercased: 505 or\n",
      "lemma: 505 or\n",
      "shape: 21581 xx\n",
      "prefix: 2491 o\n",
      "suffix: 505 or\n",
      "log probability: -5.654984951019287\n",
      "Brown cluster id: 404\n",
      "----------------------------------------\n",
      "original: 845 rather\n",
      "lowercased: 845 rather\n",
      "lemma: 845 rather\n",
      "shape: 53740 xxxx\n",
      "prefix: 5332 r\n",
      "suffix: 581 her\n",
      "log probability: -8.312031745910645\n",
      "Brown cluster id: 6698\n",
      "----------------------------------------\n",
      "original: 416 ,\n",
      "lowercased: 416 ,\n",
      "lemma: 416 ,\n",
      "shape: 416 ,\n",
      "prefix: 416 ,\n",
      "suffix: 416 ,\n",
      "log probability: -3.4549596309661865\n",
      "Brown cluster id: 4\n",
      "----------------------------------------\n",
      "original: 469 a\n",
      "lowercased: 469 a\n",
      "lemma: 469 a\n",
      "shape: 1896 x\n",
      "prefix: 469 a\n",
      "suffix: 469 a\n",
      "log probability: -3.92978835105896\n",
      "Brown cluster id: 19\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the tokens\n",
    "# All you have to do is iterate through the parsedData\n",
    "# Each token is an object with lots of different properties\n",
    "# A property with an underscore at the end returns the string representation\n",
    "# while a property without the underscore returns an index (int) into spaCy's vocabulary\n",
    "# The probability estimate is based on counts from a 3 billion word corpus, smoothed using the Simple Good-Turing method.\n",
    "for i, token in enumerate(parsedData):\n",
    "    print(\"original:\", token.orth, token.orth_)\n",
    "    print(\"lowercased:\", token.lower, token.lower_)\n",
    "    print(\"lemma:\", token.lemma, token.lemma_)\n",
    "    print(\"shape:\", token.shape, token.shape_)\n",
    "    print(\"prefix:\", token.prefix, token.prefix_)\n",
    "    print(\"suffix:\", token.suffix, token.suffix_)\n",
    "    print(\"log probability:\", token.prob)\n",
    "    print(\"Brown cluster id:\", token.cluster)\n",
    "    print(\"----------------------------------------\")\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is an art, it says, or rather, a knack to flying.\n",
      "The knack lies in learning how to throw yourself at the ground and miss.\n",
      "In the beginning the Universe was created.\n",
      "This has made a lot of people very angry and been widely regarded as a bad move.\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the sentences\n",
    "sents = []\n",
    "# the \"sents\" property returns spans\n",
    "# spans have indices into the original string\n",
    "# where each index value represents a token\n",
    "for span in parsedData.sents:\n",
    "    # go from the start to the end of each span, returning each token in the sentence\n",
    "    # combine each token using join()\n",
    "    sent = ''.join(parsedData[i].string for i in range(span.start, span.end)).strip()\n",
    "    sents.append(sent)\n",
    "\n",
    "for sentence in sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There ADV\n",
      "is VERB\n",
      "an DET\n",
      "art NOUN\n",
      ", PUNCT\n",
      "it NOUN\n",
      "says VERB\n",
      ", PUNCT\n",
      "or CONJ\n",
      "rather ADV\n",
      ", PUNCT\n",
      "a DET\n",
      "knack NOUN\n",
      "to ADP\n",
      "flying NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the part of speech tags of the first sentence\n",
    "for span in parsedData.sents:\n",
    "    sent = [parsedData[i] for i in range(span.start, span.end)]\n",
    "    break\n",
    "\n",
    "for token in sent:\n",
    "    print(token.orth_, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The det boy [] []\n",
      "boy nsubj ran ['The'] ['with']\n",
      "with prep boy [] ['dog']\n",
      "the det dog [] []\n",
      "spotted amod dog [] []\n",
      "dog pobj with ['the', 'spotted'] []\n",
      "quickly advmod ran [] []\n",
      "ran ROOT ran ['boy', 'quickly'] ['after', '.']\n",
      "after prep ran [] ['firetruck']\n",
      "the det firetruck [] []\n",
      "firetruck pobj after ['the'] []\n",
      ". punct ran [] []\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the dependencies of this example:\n",
    "example = \"The boy with the spotted dog quickly ran after the firetruck.\"\n",
    "parsedEx = parser(example)\n",
    "# shown as: original token, dependency tag, head word, left dependents, right dependents\n",
    "for token in parsedEx:\n",
    "    print(token.orth_, token.dep_, token.head.orth_, [t.orth_ for t in token.lefts], [t.orth_ for t in token.rights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "'s (not an entity)\n",
      "stocks (not an entity)\n",
      "dropped (not an entity)\n",
      "dramatically (not an entity)\n",
      "after (not an entity)\n",
      "the (not an entity)\n",
      "death (not an entity)\n",
      "of (not an entity)\n",
      "Steve PERSON\n",
      "Jobs PERSON\n",
      "in (not an entity)\n",
      "October DATE\n",
      ". (not an entity)\n",
      "-------------- entities only ---------------\n",
      "349 ORG Apple\n",
      "346 PERSON Steve Jobs\n",
      "356 DATE October\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the named entities of this example:\n",
    "example = \"Apple's stocks dropped dramatically after the death of Steve Jobs in October.\"\n",
    "parsedEx = parser(example)\n",
    "for token in parsedEx:\n",
    "    print(token.orth_, token.ent_type_ if token.ent_type_ != \"\" else \"(not an entity)\")\n",
    "\n",
    "print(\"-------------- entities only ---------------\")\n",
    "# if you just want the entities and nothing else, you can do access the parsed examples \"ents\" property like this:\n",
    "ents = list(parsedEx.ents)\n",
    "for entity in ents:\n",
    "    print(entity.label, entity.label_, ' '.join(t.orth_ for t in entity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy is trained to attempt to handle messy data, including emoticons and other web-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lol NOUN lol\n",
      "that ADJ that\n",
      "is VERB be\n",
      "rly ADV rly\n",
      "funny ADJ funny\n",
      ":) PUNCT :)\n",
      "This DET this\n",
      "is VERB be\n",
      "gr8 VERB gr8\n",
      "i NOUN i\n",
      "rate VERB rate\n",
      "it NOUN it\n",
      "8/8 NUM 8/8\n",
      "! PUNCT !\n",
      "! PUNCT !\n",
      "! PUNCT !\n"
     ]
    }
   ],
   "source": [
    "messyData = \"lol that is rly funny :) This is gr8 i rate it 8/8!!!\"\n",
    "parsedData = parser(messyData)\n",
    "for token in parsedData:\n",
    "    print(token.orth_, token.pos_, token.lemma_)\n",
    "    \n",
    "# it does pretty well! Note that it does fail on the token \"gr8\", taking it as a verb rather than an adjective meaning \"great\"\n",
    "# and \"lol\" probably isn't a noun...it's more like an interjection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy has word vector representations built in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'spacy.lexeme.Lexeme' object has no attribute 'has_repvec'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-057dbaf9e7fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# gather all known words, take only the lowercased versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mallWords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_repvec\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morth_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower_\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"nasa\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# sort by similarity to NASA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-057dbaf9e7fe>\u001b[0m in \u001b[0;36m<setcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# gather all known words, take only the lowercased versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mallWords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_repvec\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morth_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower_\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"nasa\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# sort by similarity to NASA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'spacy.lexeme.Lexeme' object has no attribute 'has_repvec'"
     ]
    }
   ],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# you can access known words from the parser's vocabulary\n",
    "nasa = parser.vocab['NASA']\n",
    "\n",
    "# cosine similarity\n",
    "cosine = lambda v1, v2: dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "\n",
    "# gather all known words, take only the lowercased versions\n",
    "allWords = list({w for w in parser.vocab if w.has_repvec and w.orth_.islower() and w.lower_ != \"nasa\"})\n",
    "\n",
    "# sort by similarity to NASA\n",
    "allWords.sort(key=lambda w: cosine(w.repvec, nasa.repvec))\n",
    "allWords.reverse()\n",
    "print(\"Top 20 most similar words to NASA:\")\n",
    "for word in allWords[:20]:   \n",
    "    print(word.orth_)\n",
    "    \n",
    "# Let's see if it can figure out this analogy\n",
    "# Man is to King as Woman is to ??\n",
    "king = parser.vocab['king']\n",
    "man = parser.vocab['man']\n",
    "woman = parser.vocab['woman']\n",
    "\n",
    "result = king.repvec - man.repvec + woman.repvec\n",
    "\n",
    "# gather all known words, take only the lowercased versions\n",
    "allWords = list({w for w in parser.vocab if w.has_repvec and w.orth_.islower() and w.lower_ != \"king\" and w.lower_ != \"man\" and w.lower_ != \"woman\"})\n",
    "# sort by similarity to the result\n",
    "allWords.sort(key=lambda w: cosine(w.repvec, result))\n",
    "allWords.reverse()\n",
    "print(\"\\n----------------------------\\nTop 3 closest results for king - man + woman:\")\n",
    "for word in allWords[:3]:   \n",
    "    print(word.orth_)\n",
    "    \n",
    "# it got it! Queen!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You can do cool things like extract Subject, Verb, Object triples from the dependency parse if you use my code in subject_object_extraction.py. Note: Doesn't work on complicated sentences. Fails if the dependency parse is incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'subject_object_extraction'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7c9af091ade2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msubject_object_extraction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfindSVOs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# can still work even without punctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"he and his brother shot me and my sister\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfindSVOs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'subject_object_extraction'"
     ]
    }
   ],
   "source": [
    "from subject_object_extraction import findSVOs\n",
    "\n",
    "# can still work even without punctuation\n",
    "parse = parser(\"he and his brother shot me and my sister\")\n",
    "print(findSVOs(parse))\n",
    "\n",
    "# very complex sample. Only some are correct. Some are missed.\n",
    "parse = parser(\"Far out in the uncharted backwaters of the unfashionable end of the Western Spiral arm of the Galaxy lies a small unregarded yellow sun. \"\n",
    "                \"Orbiting this at a distance of roughly ninety-two million miles is an utterly insignificant little blue green planet whose ape-descended \"\n",
    "                \"life forms are so amazingly primitive that they still think digital watches are a pretty neat idea. \"\n",
    "                \"This planet has – or rather had – a problem, which was this: most of the people living on it were unhappy for pretty much of the time. \"\n",
    "                \"Many solutions were suggested for this problem, but most of these were largely concerned with the movements of small green pieces of paper, \"\n",
    "                \"which is odd because on the whole it wasn’t the small green pieces of paper that were unhappy. And so the problem remained; lots of the \"\n",
    "                \"people were mean, and most of them were miserable, even the ones with digital watches.\")\n",
    "print(findSVOs(parse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If you want to include spaCy in your machine learning it is not too difficult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------\n",
      "results:\n",
      "i h8 riting comprehensibly #skoolsux : twitter\n",
      "planets and stars and rockets and stuff : space\n",
      "accuracy: 1.0\n",
      "----------------------------------------------------------------------------------------------\n",
      "Top 10 features used to predict: \n",
      "Class 1 best: \n",
      "(-0.52882945316568808, 'planet')\n",
      "(-0.35193722024200058, 'space')\n",
      "(-0.2182990882743932, 'mar')\n",
      "(-0.2182990882743932, 'red')\n",
      "(-0.15592701188189678, 'earth')\n",
      "(-0.15592701188189678, 'launch')\n",
      "(-0.15592701188189678, 'rocket')\n",
      "(-0.14828086638908844, 'great')\n",
      "(-0.14828086638908844, 'love')\n",
      "(-0.099227865545574348, 'blue')\n",
      "Class 2 best: \n",
      "(0.41129867696688982, 'twitter')\n",
      "(0.34038563734021315, '@mention')\n",
      "(0.23401438111438694, 'lol')\n",
      "(0.23401438111438694, 'gr8')\n",
      "(0.20565012258716756, 'reddit')\n",
      "(0.20565012258716756, 'fun')\n",
      "(0.20564855437972229, 'social')\n",
      "(0.20564855437972229, 'medium')\n",
      "(0.1063712562258263, 'y')\n",
      "(0.1063712562258263, 'window')\n",
      "----------------------------------------------------------------------------------------------\n",
      "The original data as it appeared to the classifier after tokenizing, lemmatizing, stoplisting, etc\n",
      "Sample 0: ('love', 1)('space', 2)('great', 1)\n",
      "Sample 1: ('space', 1)('planet', 1)('cool', 1)('glad', 1)('exist', 1)\n",
      "Sample 2: ('lol', 1)('@mention', 1)('gr8', 1)\n",
      "Sample 3: ('twitter', 1)('reddit', 1)('fun', 1)\n",
      "Sample 4: ('planet', 1)('mar', 1)('red', 1)\n",
      "Sample 5: ('@mention', 1)('y', 1)('u', 1)('skip', 1)('window', 1)('9', 1)\n",
      "Sample 6: ('planet', 1)('rocket', 1)('launch', 1)('earth', 1)\n",
      "Sample 7: ('twitter', 1)('social', 1)('medium', 1)\n",
      "Sample 8: ('@mention', 3)('hashtag', 1)\n",
      "Sample 9: ('planet', 1)('orbit', 1)('sun', 1)('little', 1)('blue', 1)('green', 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "# A custom stoplist\n",
    "STOPLIST = set(stopwords.words('english') + [\"n't\", \"'s\", \"'m\", \"ca\"] + list(ENGLISH_STOP_WORDS))\n",
    "# List of symbols we don't care about\n",
    "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-----\", \"---\", \"...\", \"“\", \"”\", \"'ve\"]\n",
    "\n",
    "# Every step in a pipeline needs to be a \"transformer\". Define a custom transformer to clean text using spaCy\n",
    "class CleanTextTransformer(TransformerMixin):\n",
    "    \"\"\"\n",
    "    Convert text to cleaned text\n",
    "    \"\"\"\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        return [cleanText(text) for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "    \n",
    "# A custom function to clean the text before sending it into the vectorizer\n",
    "def cleanText(text):\n",
    "    # get rid of newlines\n",
    "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    \n",
    "    # replace twitter @mentions\n",
    "    mentionFinder = re.compile(r\"@[a-z0-9_]{1,15}\", re.IGNORECASE)\n",
    "    text = mentionFinder.sub(\"@MENTION\", text)\n",
    "    \n",
    "    # replace HTML symbols\n",
    "    text = text.replace(\"&amp;\", \"and\").replace(\"&gt;\", \">\").replace(\"&lt;\", \"<\")\n",
    "    \n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "# A custom function to tokenize the text using spaCy\n",
    "# and convert to lemmas\n",
    "def tokenizeText(sample):\n",
    "\n",
    "    # get the tokens using spaCy\n",
    "    tokens = parser(sample)\n",
    "\n",
    "    # lemmatize\n",
    "    lemmas = []\n",
    "    for tok in tokens:\n",
    "        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n",
    "    tokens = lemmas\n",
    "\n",
    "    # stoplist the tokens\n",
    "    tokens = [tok for tok in tokens if tok not in STOPLIST]\n",
    "\n",
    "    # stoplist symbols\n",
    "    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n",
    "\n",
    "    # remove large strings of whitespace\n",
    "    while \"\" in tokens:\n",
    "        tokens.remove(\"\")\n",
    "    while \" \" in tokens:\n",
    "        tokens.remove(\" \")\n",
    "    while \"\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\")\n",
    "    while \"\\n\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\\n\")\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def printNMostInformative(vectorizer, clf, N):\n",
    "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    topClass1 = coefs_with_fns[:N]\n",
    "    topClass2 = coefs_with_fns[:-(N + 1):-1]\n",
    "    print(\"Class 1 best: \")\n",
    "    for feat in topClass1:\n",
    "        print(feat)\n",
    "    print(\"Class 2 best: \")\n",
    "    for feat in topClass2:\n",
    "        print(feat)\n",
    "\n",
    "# the vectorizer and classifer to use\n",
    "# note that I changed the tokenizer in CountVectorizer to use a custom function using spaCy's tokenizer\n",
    "vectorizer = CountVectorizer(tokenizer=tokenizeText, ngram_range=(1,1))\n",
    "clf = LinearSVC()\n",
    "# the pipeline to clean, tokenize, vectorize, and classify\n",
    "pipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer), ('clf', clf)])\n",
    "\n",
    "# data\n",
    "train = [\"I love space. Space is great.\", \"Planets are cool. I am glad they exist in space\", \"lol @twitterdude that is gr8\", \n",
    "        \"twitter &amp; reddit are fun.\", \"Mars is a planet. It is red.\", \"@Microsoft: y u skip windows 9?\", \"Rockets launch from Earth and go to other planets.\",\n",
    "        \"twitter social media &gt; &lt;\", \"@someguy @somegirl @twitter #hashtag\", \"Orbiting the sun is a little blue-green planet.\"]\n",
    "labelsTrain = [\"space\", \"space\", \"twitter\", \"twitter\", \"space\", \"twitter\", \"space\", \"twitter\", \"twitter\", \"space\"]\n",
    "\n",
    "test = [\"i h8 riting comprehensibly #skoolsux\", \"planets and stars and rockets and stuff\"]\n",
    "labelsTest = [\"twitter\", \"space\"]\n",
    "\n",
    "# train\n",
    "pipe.fit(train, labelsTrain)\n",
    "\n",
    "# test\n",
    "preds = pipe.predict(test)\n",
    "print(\"----------------------------------------------------------------------------------------------\")\n",
    "print(\"results:\")\n",
    "for (sample, pred) in zip(test, preds):\n",
    "    print(sample, \":\", pred)\n",
    "print(\"accuracy:\", accuracy_score(labelsTest, preds))\n",
    "\n",
    "print(\"----------------------------------------------------------------------------------------------\")\n",
    "print(\"Top 10 features used to predict: \")\n",
    "# show the top features\n",
    "printNMostInformative(vectorizer, clf, 10)\n",
    "\n",
    "print(\"----------------------------------------------------------------------------------------------\")\n",
    "print(\"The original data as it appeared to the classifier after tokenizing, lemmatizing, stoplisting, etc\")\n",
    "# let's see what the pipeline was transforming the data into\n",
    "pipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer)])\n",
    "transform = pipe.fit_transform(train, labelsTrain)\n",
    "\n",
    "# get the features that the vectorizer learned (its vocabulary)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "\n",
    "# the values from the vectorizer transformed data (each item is a row,column index with value as # times occuring in the sample, stored as a sparse matrix)\n",
    "for i in range(len(train)):\n",
    "    s = \"\"\n",
    "    indexIntoVocab = transform.indices[transform.indptr[i]:transform.indptr[i+1]]\n",
    "    numOccurences = transform.data[transform.indptr[i]:transform.indptr[i+1]]\n",
    "    for idx, num in zip(indexIntoVocab, numOccurences):\n",
    "        s += str((vocab[idx], num))\n",
    "    print(\"Sample {}: {}\".format(i, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
